# Developer Guide

This document is for developers to help them understand *how* the solution works. Note: not every single control will be detailed.  (e.g. controls like back buttons will not be explained.)

## Contents
- [Tables](#tables)
  - [Transcript](#transcript)
  - [Recognized Phrases](#recognized-phrases)
  - [Speaker](#speaker)
- [Demo Transcript app (canvas)](#Demo-Transcript-app-canvas)
  - [App Properties & Settings](#App-Properties-Settings)
  - [Screens](#Screens)
  - [Main Screen](#main-screen)
    - [Properties](#properties)
    - [Controls](#main-screen-controls)
  - [Transcript Demo Screen](#Transcript-Demo-Screen)
    - [Controls](#controls-1)
- [Flows](#flows)
  - [01 - SPO - When Audio File Uploaded to SPO - Copy to Azure Blob](#01---spo---when-audio-file-uploaded-to-spo---copy-to-azure-blob)
  - [02 - Azure - When Audio File Created in Blob Storage - Create Transcript](#02---azure---when-audio-file-created-in-blob-storage---create-transcript)
  - [02b Child Flow - Loop Until Transcript Complete](#02b-child-flow---loop-until-transcript-complete)
  - [02c Child Flow - Get Transcript Results](#02c-child-flow---get-transcript-results)
  - [02d Child Flow - Parse Transcript and Load into Dataverse](#02d-child-flow---parse-transcript-and-load-into-dataverse)

[← Back to Read Me](readme.md#contents)
***
## Tables
There are three related tables in the solution:  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/482ef37b-b347-474b-860d-0ca7df4d0510)  


### Transcript
A separate record is created for each transcript generated by Azure Batch Speech to Text services.  (i.e. one record for each audio file)

#### Columns
Here is a breakdown of each custom column inclulding data type and description
| **Display Name**      | **Schema Name**           | **Data Type**                 | **Description** |
| :-----------        | :----------            | :--------                  | :---------- |
| Duration            | demo_duration          | Decimal                    | Flow converts ticks to seconds | 
| Duration in Ticks   | demo_durationinticks   | Single line of text        | Use text data type because ticks are too large to store in whole number (integer) and Big Integer type has issues when importing solution |
| Source File Format  | demo_sourcefileformat  | Single line of text (fx)   | Power FX column that displays the last three characters of the file name ( e.g. mp3 or wav) |
| Source File Name    | demo_sourcefilename    | Single line of text        | Name of the audio file uploaded |
| Source File Size    | demo_sourcefilesize    | Whole Number               | File size of the audio file |
| Source URL          | demo_sourcurl          | URL                        | SAS URL of the audio file (generated via flow) |
| Time Stamp          | demo_timestamp         | Date and time              | Time and date from the transcription services |
| Transcript          | demo_transcriptid      | Unique identifier          | Unique GUID of the this record |
| Transcript Number   | demo_transcriptnumber  | # Autonumber               | Auto generated number used as the Primary Name for the record |


### Recognized Phrases
A separate record is created for each recognized phrase determined by Azure Batch Speech-To-Text services.  
#### Columns
Here is a breakdown of each custom column including data type and description
| **Display Name**      | **Schema Name**           | **Data Type**                 | **Description** |
| :-----------        | :----------            | :--------                  | :---------- |
| Display  | demo display  | Multiple lines of text  | Actual text of the current phrase determined by Azure Batch Speech to Text servies |
| Duration in Seconds  | demo_durationinseconds  | Decimal  | Flow converts  Duration in Ticks to seconds |
| Duration in Ticks  | demo_durationinticks   | Single line of text        | Use text data type because ticks are too large to store in whole number (integer) and Big Integer type has issues when importing solution | 
| Offset in Seconds  | demo_offsetinseconds  | Decimal  | Flow converts Offset in Ticks to seconds |
| Offset in Ticks    | demo_offsetinticks |  Single of text  | Use text data type because ticks are too large to store in whole number (integer) and Big Integer type has issues when importing solution | 
| Outset  | demo_outset  | Decimal (FX) | Power FX formula column that determines the outset (or out point) for the current phrase in seconds. Forumula: ```'Duration in Seconds'+'Offset in Seconds'``` |
| Phrase Number  | demo_phrasenumber  | # Autonumber  | Auto generated number used as the Primary Name for the record |
| Recognized Phrases  | demo_recognizedphrasesid  | Unique identifier          | Unique GUID of the this record |
| Speaker  | demo_speaker |  Whole number  | When diarization is enabled for Azure Batch Speech to Text, it returns a number for each unique speaker it detects.  | 
| Speaker Lookup  | demo_speakerlookup  | Lookup |  Related table: Speaker.  This relates the phrase to a speaker record (created by the user) |
| Transcript  | demo_transcript  | Lookup | Related table: Transcript.  This relates the phrase to it's parent Transcript record |


### Speaker
User can create a record for each speaker involved in a transcript.  
#### Columns
Here is a breakdown of each custom column including data type and description
| **Display Name**      | **Schema Name**           | **Data Type**                 | **Description** |
| :-----------        | :----------            | :--------                  | :---------- |
| Name  | demo_name  | Single line of text  | Speaker name and Primary column |
| Speaker  | demo_speakerid   | Unique identifier | Unique GUID of the this record |

## Demo Transcript app (canvas)
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/60e1f755-59e2-4748-b463-d7b5233b9846)

### App Properties & Settings
Several settings and properties were changed.  Note that any preview features should **not** be used for production apps. 

**Display -> Scale To Fit**  
Set to off to allow for responsive resizing. Recommend if different form factors maybe used by users (e.g. phone, tablet, laptop)  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/0478df24-4d16-491d-955d-f353d21ae58d)

**General -> Modern controls and themes**  
Set to on to allow for modern controls/themes in the app.  Note that some modern controls are in GA and others are still in preview at this time.   
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/608b1497-841b-4e9b-bbd6-79c6b5c062ce)

[^Top](#contents)

### Screens
The canvas app has two screens: 
1. [Main Screen](#main-screen)
2. [Transcript Demo Screen](#Transcript-Demo-Screen)

Both screens use containers to help control the flow of the controls when resizing the app for different resolutions.  Their layouts are based on the Sidebar screen template.

![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/1ba43b2a-0c9e-4413-a5f2-f9153b7716f4)

[^Top](#contents)

### Main Screen
This screen is used to upload audio files and select transcripts to view/edit. 
![Screenshot of Power Apps Studio with Main Screen selected](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/64881327-c8dd-40be-b663-14290ac78cae)

#### Properties

**OnVisible**: When the screen loads, several global variables are set:
- **glbShowSpinner**: Used to show/hide loading spinner
- **glbSpinnerLabel**: Used to display next next to loading spinner
- **glbShowSuccess**: Used to show/hide the Success message when upload is completed
- **glbSelectedFileName**: Stores the file name of the selected file
- **glbSelectedTranscript**: Stores the selected transcript from from the left hand gallery (galTranscripts_Main)
- **glbCurrentPhrase**: Used on the next screen, to identify the current recogonized phrase based on the current playback point in the audio controller
- **glbMode**: Used on the next screen to toggle between Edit and View display modes via the **Edit** Button

In addition to setting variables, two controls are reset (see below for more on each control):
- **attFileToUploadMain**
- **inpTotalSpeakersMain**
  
  <a name="main-screen-controls">
#### Controls
| **Parent** | **Name** | **Description** |
| :----------- | :---------- | :---------- |
| - | cont_Main_1_Vert | |
| cont_Main_1_Vert | cont_Main_1_1_Horiz | |
| cont_Main_1_1_Horiz | spinner_Main | |
| - | cont_Main_2_Vert | |
| cont_Main_2_Vert | cont_Main_2_1_Horiz  | |
| cont_Main_2_1_Horiz | headerMain | |
| cont_Main_2_Vert | cont_Main_2_2_Horiz | |
| cont_Main_2_2_Horiz | shpSpacerLeftMain | |
| cont_Main_2_2_Horiz | cont_Main_2_2_1_Vert | |
| cont_Main_2_2_1_Vert | [attFileToUpload](#attFileToUpload) | Allows user to upload a file |
| cont_Main_2_2_1_Vert | [inpTotalSpeakersMain](#inpTotalSpeakersMain) | Number input field that indicates how many speakers should Azure Speech to Text services look for |
| cont_Main_2_2_1_Vert | cont_Main_2_2_1_1_Horiz | |
| cont_Main_2_2_1_1_Horiz | [btnUploadFile_Main](#btnUploadFile_Main) | Used to upload the selected file to Azure Blob Storage (via Power Automate flow) |
| cont_Main_2_2_1_1_Horiz | [btnCancelUpload_Main](#btnCancelUpload_Main) | Resets the controls (attFileToUploadMain, inpTotalSpeakersMain) |
| cont_Main_2_2_Horiz | cont_Main_2_2_2_Vert | |
| cont_Main_2_2_2_Vert | lblTranscript_Main | |
| cont_Main_2_2_2_Vert | [galTranscripts_Main](#galTranscripts_Main) | Displays **all** the available transcripts in the Transcripts table |
| galTranscripts_Main | cont_Main_2_2_2_1_Horiz | |
| cont_Main_2_2_2_1_Horiz | cont_Main_2_2_2_1_1_Vert | |
| cont_Main_2_2_2_1_1_Vert | lblTranscriptFileName_Main | |
| cont_Main_2_2_2_1_1_Vert | lblTranscriptDetails_Main | |
| cont_Main_2_2_2_1_1_Vert | lblTranscriptSummary | |
| cont_Main_2_2_2_1_Horiz | [btnEditTranscript_Main](#btnEditTranscript_Main) | Selects the transcript and opens it in the [Transcript Demo Screen](#transcript-demo-screen) |
| galTranscripts_Main | [btnRefreshTranscript_Main](#btnRefreshTranscript_Main)| Refreshes the **Transcripts** table |
| cont_Main_2_2_Horiz | cont_Main_2_2_3_Vert | |
| cont_Main_2_2_3_Vert | htmlSuccessMain | |
| cont_Main_2_2_3_Vert | btnBackSuccessMain | |
| cont_Main_2_2_Horiz | shpSpacerRightMain | |

**_Many controls are located inside container(s). The path/location will be indicated in paranthesis._**  

##### attFileToUpload:  
_(contAllMainHoriz/contBottomMainHoriz/contRightMainVert)_

This control allows user to upload a file. The control validates file size and file format:
- The API has a limit of 100 Mb, so the control is limited to 100 Mb.
- The Speach to Text API only accepts the following formats:
  - MP3
  - WAV
  - AAC
  - OPUS
  - OGG
  - FLAC
  - WMA
  - AMR
  - WEBM
  - M4A
  - SPEEX
    
It has several properties customized:
- **AccessibleLabel**: ```"File to attach (upload) and transcribe"```
- **AddAttachmentText**: ```"Select audio file (100 MB Max)"```
- **Color**: This changes color to red if selected file is not supported audio file
  ```
  If(
    //IF Attachment is supported format
    Lower(
        Right(
            First(attFileToUploadMain.Attachments).Name,
            3
        )
    ) in colSupportedFileFormats,
    // THEN Color is Black
    Color.Black,
    // IF no attachment is selected
    IsEmpty(Self.Attachments),
    // THEN color is black
    Color.Black,
    // ELSE (Error) color is red
    Color.Red
  )
  ```
- **Height**: ```100```
- **MaxAttachments**: ```1```
   - If you want to allow for batch uploads, increase this option, but performance may suffer for larger files. Also some other parts of the solution may need to be refactored if you allow more than 1 file at a time
- **MaxAttachmentSize**: ```1000```
   - _In MB_
- **MaxAttachmentText**: This code does some basic data validation to check if the selected file is supported format
  ```
  If(
    //IF Attachment is supported format
        Lower(
            Right(
                First(attFileToUploadMain.Attachments).Name,
                3
            )
        ) in colSupportedFileFormats,
    "File Selected. Please click Upload",
    // ELSE display error
    "Error:  Only these file formats are supported: " &  Replace(glbListSupportedFileFormats,Len(glbListSupportedFileFormats)-1,1,"")
  )
  ```
- **NoAttachmentsText**: ```"There is nothing selected."```
- **OnAddFile**: Stores the selected file in a global variable (glbSelectedFileName).
  _IF you allow for more than one attachment, you'll need to refactor this_
  ```
  Set(
    glbSelectedFileName,
    First(attFileToUploadMain.Attachments).Name
  )
  ```
- **OnRemoveFile**: When file is removed, clears variable
  ```
  Set(
    glbSelectedFileName,
    Blank()
  )
  ```
- **Width**: ```Parent.Width - 60 ```
  

##### inpTotalSpeakersMain
_(contAllMainHoriz/contBottomMainHoriz/contRightMainVert)_

Number input field that indicates how many speakers should Azure Speech to Text services look for.
- **AccessibleLabel**: ```"Enter the total number of speakers in the audio file"```
- **Max**: ```36```
  - Azure Speech To Text services has a limit of 36 speakers for diarization
- **Min**: ```1```
- **Value**: ```0``` 


##### btnUploadFile_Main 
_(contAllMainHoriz/contBottomMainHoriz/contRightMainVert/contButtonsRightMainHoriz)_

Used to upload the selected file to Azure Blob Storage (via Power Automate flow)
- **AccessibleLabel**: ```"Upload the selected file"```
- **DisplayMode**: Default mode disabled. Only enabled (Edit mode) when file is attached, the format is correct and the total speakers is greater than zero
  ```
   // If total speakers isn't set (min 1), disable this button
  If(
      inpTotalSpeakersMain.Value>0,
      // If No attachment, disable button
      If(
          IsEmpty(attFileToUploadMain.Attachments),
          DisplayMode.Disabled,
          //IF Attachment is supported format
          Lower(
              Right(
                  First(attFileToUploadMain.Attachments).Name,
                  3
              )
          ) in colSupportedFileFormats,
          // THEN Enable button
          DisplayMode.Edit,
          //ELSE disable button
          DisplayMode.Disabled
      ),
      //ELSE disable button
      DisplayMode.Disabled
  )
  ```
- **OnSelect**: When clicked, shows the loading spinner and calls the Power Automate Flow 
   ```
   // Show the loading spinner
  Set(
      glbShowSpinner,
      true
  );
  //Set loading spinner label
  Set(
      glbSpinnerLabel,
      "Uploading..."
  );
  // Store response from flow in variable (glbResponseUpload)
  Set(
      glbResponseUpload,
      //Run flow to upload the file and kickoff the transcript process
      '01-PowerApps-UploadtoAzureBlob'.Run(
          inpTotalSpeakersMain.Value,
          {
              file: {
                  name: First(attFileToUploadMain.Attachments).Name,
                  contentBytes: First(attFileToUploadMain.Attachments).Value
              }
          }
      )
  );
  // Check for error uploading file
  IfError(
      glbResponseUpload,
      //Notify user of error
      Notify(
          "Error: " & FirstError.Message,
          NotificationType.Error
      );
      //Hide the loading spinner
  Set(
          glbShowSpinner,
          false
      ),
  //IF Successful then
      Concurrent(
      //Hide the loading spinner
          Set(
              glbShowSpinner,
              false
          ),
      //Show the success message
          Set(
              glbShowSuccess,
              true
          ),
          //Reset Attachment Control
          Reset(attFileToUploadMain),
          // Reset Total Speakers input
          Reset(inpTotalSpeakersMain)
      )
  )
   ```
- **Text**: ```"Upload"```

##### btnCancelUpload_Main
_(contAllMainHoriz/contBottomMainHoriz/contRightMainVert/contButtonsRightMainHoriz)_

Resets the controls (attFileToUploadMain, inpTotalSpeakersMain)
- **OnSelect**:
  ```
  //Reset upload attachment and total number of speakers controls
  Reset(attFileToUploadMain);
  // Reset the total speakers input field
  Reset(inpTotalSpeakersMain)
  ```
- **Text**: ```"Cancel"```

##### galTranscripts_Main
_(contAllMainHoriz/contBottomMainHoriz/contSiderbarMainVert)_

Displays **all** the available transcripts in the Transcripts table. Some properties were customized:
- **AccessibleLabel**: ```"List of all the transcripts"```
- **Items**: ```SortByColumns(Transcripts,"createdon",SortOrder.Descending)```
- **LayoutMinHeight**: ```284```
- **TemplateSize**:```274```
- **Width**: ```Parent.Width-Parent.PaddingLeft-Parent.PaddingRight-Parent.LayoutGap```
  
##### btnEditTranscript_Main
_(contAllMainHoriz/contBottomMainHoriz/contSiderbarMainVert/galTranscripts_Main/contTranscriptsHoriz/)_

- **AccessibleLabel**: ```"Click view and edit this transcript"```
- **Appearance**:```'ButtonCanvas.Appearance'.Secondary```
- **Icon**: ```"MoreHorizontal"```
- **Layout**:```'ButtonCanvas.Layout'.IconAfter```
- **OnSelect**: When item is selected, store a copy of the **Recognized Phrases** for the selected transcript in a collection (**colPhrases**) and sort the collection in acensinding order by the '**Offset in Seconds**', then store the selected Transcript record in a global varilable (**glbSelectedTranscript**), then set a variable (**glbCurrentPhrase**) to the first phrase of colPhrases,  then go to the **Transcript Demo Screen**  
   ```
   //Set spinner label and show spinner
  Set(
      glbSpinnerLabel,
      "Loading..."
  );
  Set(
      glbShowSpinner,
      true
  );
  //When item is selected
  //Store a copy of the Recognized Phrases for the selected transcript in a collection (colPhrases)
  // And sort the collection in acensinding order by the 'Offset in Seconds'
  ClearCollect(
      colPhrases,
      SortByColumns(
          Filter(
              ShowColumns(
                  'Recognized Phrases',
                  Display,
                  'Duration in Seconds',
                  'Duration in Ticks',
                  'Offset (HH:MM:SS)',
                  'Offset in Seconds',
                  'Offset in Ticks',
                  Outset,
                  'Outset (HH:MM:SS)',
                  'Phrase Number',
                  Speaker,
                  'Speaker Lookup',
                  Transcript,
                  'Recognized Phrases'
              ),
              demo_Transcript.Transcript = ThisItem.Transcript
          ),
          "demo_offsetinseconds",
          SortOrder.Ascending
      )
  );
  //Store the currently selected transcript in a global variable
  Set(
      glbSelectedTranscript,
      ThisItem
  );
  //Set current phrase (glbCurrentPhrase) to the first item in the phrases collection (colPhrases)
  Set(
      glbCurrentPhrase,
      First(colPhrases)
  );
  //Navigate to the Transcript Demo Screen
  Navigate('Transcript Demo Screen');
  //Hide Spinner
  Set(
      glbShowSpinner,
      false
  );

   ```
- **Text**: ```Details```
- **Width**:```100```

##### btnRefreshTranscript_Main
_(contAllMainHoriz/contBottomMainHoriz/contSiderbarMainVert)_

Refreshes the **Transcripts** table
- **AccessibleLabel**:```"Refresh the list of transcripts"```
- **AlignInContainer**:```AlignInContainer.Center```
- **Appearance**:```'ButtonCanvas.Appearance'.Secondary```
- **Icon**:```"ArrowClockwise"```
- **Text**:```"Refresh"```
- **OnSelect**:```Refresh(Transcripts)```

[^Top](#contents)

### Transcript Demo Screen
This screen has several containers. Some of these are used to for pop-up windows, while most are used to structure the controls.  

![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/1da2865b-c81e-433a-851c-5fba650bbbfc)

#### Controls

All controls (except two) are stored in horizontal and vertical containers to allow for responsive design when the user's screen resolution and aspect ratio change.  Briefly, here are the controls and what they do. Click the control for more details:

| **Parent** | **Control** | **Description** |
| :-----------   | :-----------   | :-----------   |
| - | **contSpinnerTranscriptBg** | Contains the loading spinner and is only visible when **glbShowSpinner** = true  |
| - | **pdfFileTranscript** | Displays the PDF version of the transcript. Only visible when Transcript File is attached to record and the **Trancript (PDF)** tab is selected |
| - | **[timerTranscript](#timerTranscript)** | Used to update variables based on the playhead of the audio control (**audRecordingPlayback**). Some of the properties have been customized |
| - |  **contMainTranscriptVert** | Contains the main UI for this screen including playback and edit controls |
| contMainTranscriptVert | tabMainTranscript | |
| contMainTranscriptVert | contMainBodyTranscriptHoriz | |
| contMainBodyTranscriptHoriz | contCommandBartTranscriptHoriz | |
| contCommandBartTranscriptHoriz | **[btnEdit_Transcript](#btnEdit_Transcript)** | Used to put screen into edit mode |
| contCommandBartTranscriptHoriz | **[btnSave_Transcript](#btnSave_Transcript)** | Used to save changes |
| contCommandBartTranscriptHoriz | **[btnCancel_Transcript](#btnCancel_Transcript)** | Resets screen to display mode without saving changes|
| contMainBodyTranscriptHoriz | **[txtSummaryTranscript](#txtSummaryTranscript)** | Used to display and edit AI generated summary of transcript |
| contMainBodyTranscriptHoriz | contFileTranscript | |
| contFileTranscript | shpFileTranscript | |
| contFileTranscript | contFileTranscriptHoriz | |
| contFileTranscriptHoriz | btnDownloadTranscriptFile | |
| contFileTranscriptHoriz | btnRefreshTranscriptFile | |
| contMainBodyTranscriptHoriz | contMainBodyTranscriptVert | |
| contMainBodyTranscriptVert | **[audRecordingPlayback](#audRecordingPlayback)** | Used to playback the original audio (stored in Azure Blob Storage) |
| contMainBodyTranscriptVert | contMainSubTranscriptHoriz | |
| contMainSubTranscriptHoriz | contAllPhrasesTranscriptVert | |
| contAllPhrasesTranscriptVert | galAllPhrasesTranscript | |
| contMainBodyTranscriptVert | contMainSubTranscriptVert | |
| contMainSubTranscriptVert | txtCurrentPhrase_Transcript | |
| contMainSubTranscriptVert | contSpeakerTranscriptHoriz | |
| contSpeakerTranscriptHoriz | **[drpSelectSpeaker_Transcript](#drpSelectSpeaker_Transcript)** | Used to select speaker from **Speakers** table |
| contSpeakerTranscriptHoriz | **[icoClearSelectSpeaker_Transcript](#icoClearSelectSpeaker_Transcript)** | Resets **[drpSelectSpeaker_Transcript](#drpSelectSpeaker_Transcript)** |
| contSpeakerTranscriptHoriz | **[btnNewSpeaker_Transcript ](#btnNewSpeaker_Transcript )** | Opens the **[contPopUpUpdateAllSpeakersBg](#contPopUpUpdateAllSpeakersBg)** pop-up |
| contSpeakerTranscriptHoriz | **[lblCurrentSpeaker_Transcript](#lblCurrentSpeaker_Transcript)** | Displays either Speaker name (if available) or speaker number (as generated by Azure Speech to Text) |
| contMainTranscriptVert | contDetailsTranscriptHoriz | |
| contDetailsTranscriptHoriz | **[btnJumpToInPoint ](#btnJumpToInPoint )** |  |
| contDetailsTranscriptHoriz | **[lblInPoint_Transcript ](#lblInPoint_Transcript )** |  |
| contDetailsTranscriptHoriz | **[lblOutpoint_Transcript:](#lblOutpoint_Transcript)** |  |
| contDetailsTranscriptHoriz | **[lblJumpToTime_Transcript](#lblJumpToTime_Transcript)** |  |
| contDetailsTranscriptHoriz | **[txtJumpToTime_Transcript ](#txtJumpToTime_Transcript )** | Allows user to type time code (HH:MM:SS) to jump to part of recording (and transcript) |
| - | **[contPopUpAddSpeaker](#contPopUpAddSpeaker)** | Only visible when **glbShowPopUpAddSpeaker** = true  |
| contPopUpAddSpeaker | **[frmAddSpeaker ](#frmAddSpeaker )** |  | Submits new speaker name to Speakers table |
| contPopUpAddSpeaker | contPopUpAddSpeakerButtons | |
| contPopUpAddSpeakerButtons | **[btnAddSpeakerSave ](#btnAddSpeakerSave )** | |
| contPopUpAddSpeakerButtons | **[btnAddSpeakerCancel ](#btnAddSpeakerCancel )** | |
| - | **[contPopUpUpdateAllSpeakersBg](#contPopUpUpdateAllSpeakersBg)** | Is only visible when **gblShowPopUpUpdateAllSpeakers** = true   |
| contPopUpUpdateAllSpeakersBg | **[contPopUpUpdateAllSpeakers ](#contPopUpUpdateAllSpeakers )** | |
| contPopUpUpdateAllSpeakers | contPopUpUpdateAllSpeakersButtons | |
| contPopUpUpdateAllSpeakersButtons | **[btnPopUpUpdateAllSpeakersYes](#btnPopUpUpdateAllSpeakersYes)** | |
| contPopUpUpdateAllSpeakersButtons | **[btnPopUpUpdateAllSpeakersNo](#btnPopUpUpdateAllSpeakersNo)** | |
| contPopUpUpdateAllSpeakersButtons | **[btnSaveHidden](#btnSaveHidden)** |  | This button is hidden, but is called by various other buttons. This is one technique to create reusable code/functions in Power Apps |
| contMainBodyTranscriptVert | contMainSubTranscriptHoriz | |
| contMainSubTranscriptHoriz | contMainSubTranscriptVert | |
| contMainSubTranscriptVert | **[txtCurrentPhrase_Transcript](#txtCurrentPhrase_Transcript)** |  Displays the text of the currently selected phrase |

----------

##### pdfFileTranscript
Displays the PDF version of the transcript. Only visible when Transcript File is attached to record and the Trancript (PDF) tab is selected

- **Document**: ```LookUp(Transcripts,Transcript=glbSelectedTranscript.Transcript).'Transcript File'.Value```
- **Height**:```shpFileTranscript.Height```
- **Visible**: Only displays when selected transcript has PDF attached and "Transcript (PDF)" tab is selected
    ```
  !IsBlank(
      LookUp(
          Transcripts,
          Transcript = glbSelectedTranscript.Transcript
      ).'Transcript File'.Value
  ) And tabMainTranscript.Selected.Value = "Transcript (PDF)"
    ```
- **Width**:```contFileTranscript.Width-contFileTranscript.RadiusBottomRight*2```
- **x**:```contMainBodyTranscriptHoriz.X+contMainBodyTranscriptHoriz.RadiusBottomLeft```
- **y**:```contMainBodyTranscriptHoriz.Y+contMainBodyTranscriptHoriz.RadiusBottomLeft```
- **Zoom**:```Zoom.FitHeight```

##### timerTranscript   

- **Duration**: This is in milliseconds. 1000 = 1 second  
  ```1000```
- **OnTimerStart**: Every second, update the current phrase (glbCurrentPhrase)
  ```
  Set(
      glbCurrentPhrase,
      LookUp(
          ShowColumns(
              colPhrases,
              Display,
              'Offset in Seconds',
              Outset,
              'Phrase Number',
              Speaker,
              'Speaker Lookup',
              Transcript,
              'Recognized Phrases',
              'Duration in Seconds',
              'Offset (HH:MM:SS)',
              'Outset (HH:MM:SS)'
          ),
          // Current phrase is between the offset in seconds and the outset
          demo_offsetinseconds <= Trunc(audRecordingPlayback.Time) And demo_outset >= Trunc(audRecordingPlayback.Time)
      )
  )
   ```
- **Repeat**: ```true```
- **Start**: ```glbStartTimer```
- **Visible**: ```false```

##### txtSummaryTranscript 
_(contMainTranscriptVert/contMainBodyTranscriptHoriz/contMainBodySummaryTranscriptVert)_

Used to display and edit AI generated summary of transcript
- **AccessibleLabel**:```"AI Generated summary using Azure OpenAI"```
- **Appearance**:```If(glbMode=DisplayMode.Edit,'TextInputCanvas.Appearance'.FilledDarker,'TextInputCanvas.Appearance'.FilledLighter)```
- **DisplayMode**:```glbMode```
- **Mode**:``````
- ****:```'TextInputCanvas.Mode'.Multiline```
- **Value**:```LookUp(Transcripts,Transcript=glbSelectedTranscript.Transcript).Summary```
- **Width**:```Parent.Width-Parent.PaddingLeft-Parent.PaddingRight```

##### audRecordingPlayback  
_(contMainTranscriptVert->contMainBodyTranscriptHoriz->contMainBodyTranscriptVert)_  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/4d4feed5-64ae-4b99-bb2c-9d0fe8815037)

Used to playback the original audio (stored in Azure Blob Storage)
- **AccessibleLabel**:```"Playback control for the selected transcript's original audio recording"```
- **DisplayMode**: If user is editing the current phrase, disable this so they can't move the playhead (and change the current phrase)
  ```
  If(
    glbMode = DisplayMode.Edit,
    DisplayMode.Disabled,
    DisplayMode.Edit
   )
   ```
- **Fill**: ```PowerAppsTheme.Colors.Primary```
  - Note: PowerAppsTheme is the default theme.  You can replace the default theme with your own.  
     ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/05fc98ea-a851-426d-b878-1ca3d53fea08)
- **Media**: ```glbSelectedTranscript.'Source URL'```
 - **OnEnd**: ```Set(glbStartTimer,false);```
 - **OnPause**: ```Set(glbStartTimer,false);```
 - **OnStart**: ```Set(glbStartTimer,true);```
 - **StartTime**: ```glbJumpToTime```
 - **Width**: ```Parent.Width```

##### btnEdit_Transcript
_(contMainTranscriptVert->contFooterTranscriptHoriz)_  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/21d4b135-ab8f-4761-8b3f-9527367f5cd6)

_Note: only visible when **NOT** in edit mode and glbSelectedTranscript is NOT blank_
- **AccessibleLabel**: ```"Edit the current phrase"```
- **Appearance**:```'ButtonCanvas.Appearance'.Transparent```
- **DisplayMode**:```If (tabMainTranscript.Selected.Value="Transcript (PDF)",DisplayMode.Disabled,DisplayMode.Edit)```
- **Icon**:```"Edit"```
- **IconStyle**:```'ButtonCanvas.IconStyle'.Outline```
- **OnSelect**:
  ```
  Set(
    glbMode,
    DisplayMode.Edit
  )
  ```
- **Text**:```"Edit"```
- **Visible** ```Not(glbMode=DisplayMode.Edit) And !IsBlank(glbSelectedTranscript)```

##### btnSave_Transcript
_(contMainTranscriptVert->contFooterTranscriptHoriz)_   
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/6b7670e6-c99f-4693-9158-2262df8cd618)

_Note: only visible when in edit mode_  
- **AccessibleLabel**: ```"Save edits to current phrase"```
- **Appearance**:```'ButtonCanvas.Appearance'.Transparent```
- **Icon**:```"Save"```
- **IconStyle**:```'ButtonCanvas.IconStyle'.Outline```
- **OnSelect**: Button behaves differently depending on the tab selected. See comments for more details 
  ```
   If(
      //If Playback tab is selected
      tabMainTranscript.Selected.Value = "Playback",
      //Display popup and determine if every instance of the current Speaker value (e.g. 1) should be set to the selected speaker value for all records in this transcript
      If(
          !IsBlank(drpSelectSpeaker_Transcript.Selected),
          Set(
              gblShowPopUpUpdateAllSpeakers,
              true
          ),
      //Otherwise just save changes
          Select(btnSaveHidden)
      ),
      // If Summary tab is selected
      tabMainTranscript.Selected.Value = "Summary",
      //Set spinner label
      Set(glbSpinnerLabel,"Saving");
      // Show spinnner
      Set(
          glbShowSpinner,
          true
      );
      //Update current Transcript record with updated summary and update variable (glbSelectedTranscript) with result
  Set(
          glbSelectedTranscript,
          Patch(
              Transcripts,
              LookUp(
                  Transcripts,
                  Transcript = glbSelectedTranscript.Transcript
              ),
              {Summary: txtSummaryTranscript.Value}
          )
      );
      // If No Errors...
  If(
          IsEmpty(Errors(Transcripts)),
      //.... Reset display mode to View
          Set(
              glbMode,
              DisplayMode.View
          )
      );
      //Hide Spinner
  Set(
          glbShowSpinner,
          false
      );
      
  )
  ```
- **Text**: ```"Save"```
- **Visible**: ```glbMode=DisplayMode.Edit```

##### btnCancel_Transcript
_(contMainTranscriptVert->contFooterTranscriptHoriz)_     
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/c26cdcec-d1a9-494e-b872-d759a875108d)

  _Note: Only visible when in Edit mode_  
- **AccessibleLabel**:```"Cancel the edits to the current phrase"```
- **Appearance**: ```'ButtonCanvas.Appearance'.Outline```
- **Icon**:```"Dismiss"```
- **IconStyle**:```"Outline"```
- **OnSelect**: Resets controls and app to View mode
  ```
  Set(
    glbMode,
    DisplayMode.View
  );
  Reset(drpSelectSpeaker_Transcript);
  Reset(txtCurrentPhrase_Transcript)
  ```
- **Text**: ```"Cancel"```
- **Visible**: ```glbMode=DisplayMode.Edit```

##### txtCurrentPhrase_Transcript** 
_(contMainTranscriptVert->contMainBodyTranscriptHoriz->contMainBodyTranscriptVert)_  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/98c3429a-cd78-4f37-b203-525eab121cf1)

- **AccessibleLabel**: ```"Transcript of the current phrase (based current time code)"```
- **DisplayMode**: If variable glbMode is blank, default to View mode
  ```Coalesce(glbMode,DisplayMode.View)```
- **FontSize**:```20```
- **Mode**: ```'TextInputCanvas.Mode'.Multiline```
- **Value**: Return the current phrase (glbCurrentPhrase) Display column value
  ```
  glbCurrentPhrase.Display
  ```
- **Width**: ```Parent.Width```

##### lblCurrentSpeaker_Transcript
_(contMainTranscriptVert->contMainBodyTranscriptHoriz->contMainBodyTranscriptVert->contSpeakerTranscriptHoriz)_  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/22d1f4da-33fc-46bc-9526-fd5e9653ae52)

- **Text**: If the speaker dropdown has a selected name, use that. If not, use the value of the speaker name from the current phrase (glbCurrentPhrase). If no name exists, get the speaker value (number) from the current phrase
  ```
  "Speaker: " & Coalesce(
    drpSelectSpeaker_Transcript.Selected.Name,
    glbCurrentPhrase.demo_SpeakerLookup.Name,
    glbCurrentPhrase.demo_speaker
  )
  '''
- **Visible**: ```!IsBlank(glbCurrentPhrase)```

##### lblSelectSpeaker_Transcript
_(contMainTranscriptVert->contMainBodyTranscriptHoriz->contMainBodyTranscriptVert->contSpeakerTranscriptHoriz)_  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/4d0bf4e0-766c-4cb1-ac55-da19173be8c6)

  _Note: only visible when app is in Edit mode_  
- **Align**: ```'TextCanvas.Align'.End```
- **Text**: ```"Select Speaker"```
- **Visible**: ```glbMode=DisplayMode.Edit```

#####  drpSelectSpeaker_Transcript
_(contMainTranscriptVert->contMainBodyTranscriptHoriz->contMainBodyTranscriptVert->contSpeakerTranscriptHoriz)_  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/8e22e6c4-70e6-4328-8a15-2e41d61326d8)

- **AccessibleLabel**: ```"Select speaker from this drop down"```
- **Items**: ```Filter(Speakers, Transcript.Transcript=glbSelectedTranscript.Transcript)```
  - _Only returns speakers already related to current transcript._
- **Visible**: ```glbMode=DisplayMode.Edit```
  - _Only visible when app is in Edit mode_    

##### icoClearSelectSpeaker_Transcript
_(contMainTranscriptVert->contMainBodyTranscriptHoriz->contMainBodyTranscriptVert->contSpeakerTranscriptHoriz)_  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/c58b5e2f-3201-4899-8fb8-263c48b76709)

  _Note: only visible when speaker is selected in dropdown_  
- **AccessibleLabel**: ```"Clear selected speaker dropdown"```
- **BorderStyle**: ```BorderStyle.None```
- **Color**: ```PowerAppsTheme.Colors.Primary```
- **Fill**: ```ColorValue("#F5F5F5")```
- **Icon**: ```Icon.Cancel```
- **Height**: ```drpSelectSpeaker_Transcript.Height```
- **OnSelect**: ```Reset(drpSelectSpeaker_Transcript)```
- **PaddingTop**: ```10```
- **PaddingBottom**, **PaddingRight**, **PaddingLeft**: ```Self.PaddingTop```
- **Visible**: ```!IsBlank(drpSelectSpeaker_Transcript.Selected)```
- **Width**: ```Self.Height```

**btnNewSpeaker_Transcript** 
_(contMainTranscriptVert->contMainBodyTranscriptHoriz->contMainBodyTranscriptVert->contSpeakerTranscriptHoriz)_  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/08151b97-e2bb-404b-9638-b85eed23d579)

- **AccessibleLabel**: ```"Add new speaker"```
- **Appearance**: ```'ButtonCanvas.Appearance'.Subtle```
- **OnSelect**: Show the Add Speaker Pop Up
  ```
  Set(
      glbShowPopUpAddSpeaker,
      true
  )
  ```
- **Text**: ```"+ New Speaker"```
- **Visible**: ```glbMode=DisplayMode.Edit```

**lblSourceFileName_Transcript** 
_(contMainTranscriptVert->contMainBodyTranscriptHoriz->contMainBodyTranscriptVert->contDetailsTranscriptHoriz)_  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/465e84fc-d20b-41ea-b3c0-cb8d5c7f876b)

- **FillPortions**: ```4```
- **Text**: ```"Source: " & glbSelectedTranscript.'Source File Name'```

**btnJumpToInPoint** 
_(contMainTranscriptVert->contMainBodyTranscriptHoriz->contMainBodyTranscriptVert->contDetailsTranscriptHoriz)_    
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/28f01fac-1537-425b-99f8-f7c3c532327f)  

- **AccessibleLabel**: ```"Jump to In Point"```
- **Appearance**: ```'ButtonCanvas.Appearance'.Outline```
- **OnSelect**: ```Set(glbJumpToTime,glbCurrentPhrase.demo_offsetinseconds)```
- **Text**: ```"↦"1```
- **Visible**: ```glbMode=DisplayMode.View```

**lblInPoint_Transcript** 
_(contMainTranscriptVert->contMainBodyTranscriptHoriz->contMainBodyTranscriptVert->contDetailsTranscriptHoriz)_  
 ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/483a374f-8295-48de-b13d-a6f060a828ed)  
 - **Text**: Display the current phrase's in point (Offset in Seconds) in HH:MM:SS format
   ```
   " In: " & Text(
    RoundDown(
        glbCurrentPhrase.demo_offsetinseconds / 3600,
        0
    ),
    "00"
    ) & ":" & Text(
        RoundDown(
            If(
                Mod(
                    glbCurrentPhrase.demo_offsetinseconds,
                    3600
                ) > 0,
                Mod(
                    glbCurrentPhrase.demo_offsetinseconds,
                    3600
                ) / 60,
                glbCurrentPhrase.demo_offsetinseconds / 60
            ),
            0
        ),
        "00"
    ) & ":" & Text(
        Mod(
            glbCurrentPhrase.demo_offsetinseconds,
            60
        ),
        "00"
      )
    ```
- **Visible**: ```!IsBlank(glbCurrentPhrase)```
- **Width**: ```120```

**lblOutpoint_Transcript**: 
_(contMainTranscriptVert->contMainBodyTranscriptHoriz->contMainBodyTranscriptVert->contDetailsTranscriptHoriz)_  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/d11bac0b-e3fc-4b3b-8dbf-72ce79db931a)  
- **Align**: ```'TextCanvas.Align'.End```
- **Text**: Display current phrase's out point (outset) in HH:MM:SS
  ```
   "Out: " & Text(
      RoundDown(
          glbCurrentPhrase.demo_outset / 3600,
          0
      ),
      "00"
  ) & ":" & Text(
      RoundDown(
          If(
              Mod(
                  glbCurrentPhrase.demo_outset,
                  3600
              ) > 0,
              Mod(
                  glbCurrentPhrase.demo_outset,
                  3600
              ) / 60,
              glbCurrentPhrase.demo_outset / 60
          ),
          0
      ),
      "00"
  ) & ":" & Text(
      Mod(
          glbCurrentPhrase.demo_outset,
          60
      ),
      "00"
  ) & " "
  ```
- **Visible**: ```!IsBlank(glbCurrentPhrase)```
- **Width**: ```120```

**lblJumpToTime_Transcript** _(contMainTranscriptVert->contMainBodyTranscriptHoriz->contMainBodyTranscriptVert->contDetailsTranscriptHoriz)_  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/1a360425-3a9f-48af-af57-909b9ed26a7e)  
- **Align**: ```'TextCanvas.Align'.End```
- **FontColor**: If variable glbJumpToTime exceeds the total duration of the audio file, display red text
  ```
  If(
    glbJumpToTime > RoundUp(
        glbSelectedTranscript.Duration,
        0
    ),
    Color.Red,
    Color.Black
  )
  ```
- **Text**: If variable glbJumpToTime exceeds the total duration of the audio file, display error message, otherwise "Jump To"
  ```
  If(
    glbJumpToTime > RoundUp(
        glbSelectedTranscript.Duration,
        0
    ),
    "Cannot exceed total duration ",
    "Jump To "
  )
  ```
**txtJumpToTime_Transcript** _(contMainTranscriptVert->contMainBodyTranscriptHoriz->contMainBodyTranscriptVert->contDetailsTranscriptHoriz)_  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/d847eef6-59ae-4fa9-bcd6-d2b8efd336d1)  
Allows user to type time code (HH:MM:SS) to jump to part of recording (and transcript)

- **AccessibleLabel**: ```"Type the time you want to jump to (Hours:Minutes:Seconds)"```
- **FontColor**:
  ```
  If(
    glbJumpToTime > RoundUp(
        glbSelectedTranscript.Duration,
        0
    ),
    Color.Red,
    Color.Black
  )
  ```
- **OnChange**: 
  ```
  //If string value doesn't equal 8 (the length of the string 00:00:00), do nothing
  If(
      Len(Self.Value) = 8,
      //Convert string (HH:MM:SS format) into seconds and store in variable glbJumpToTime
      Set(
          glbJumpToTime,
          Value(
              Left(
                  Self.Value,
                  2
              )
          ) * 3600 + Value(
              Mid(
                  Self.Value,
                  4,
                  2
              )
          ) * 60 + Value(
              Right(
                  Self.Value,
                  2
              )
          )
      );
      //Refresh current phrase
      Set(
          glbCurrentPhrase,
          LookUp(
              colPhrases,
          // Current phrase is between the offset in seconds and the outset
              demo_offsetinseconds <= Trunc(audRecordingPlayback.Time) And demo_outset >= Trunc(audRecordingPlayback.Time)
          )
      )
  )
  ```
- **Value**: Display current playback time in HH:MM:SS
  ```
  Text(
      RoundDown(
          audRecordingPlayback.Time / 3600,
          0
      ),
      "00"
  ) & ":" & Text(
      RoundDown(
          audRecordingPlayback.Time / 60,0
      ),
      "00"
  ) & ":" & Text(
      Mod(
          audRecordingPlayback.Time,
          60
      ),
      "00"
  )
  ```
- **Width**: ```100```

<a name="contPopUpAddSpeaker"></a>
**contPopUpAddSpeaker**  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/edbb1d88-5c0b-46b0-9255-ef5f0dceb74e)

Only visible when user clicks + New Speaker button (btnNewSpeaker_Transcript)
- **Fill**: ```RGBA(255, 255, 255, 1)```
- **Height**: ```txtCurrentPhrase_Transcript.Height-10```
- **PaddingLeft**: ```10```
- **PaddingRight**: ```5```
- **PaddingTop**: ```5```
- **Border Radius** (**RadiusBottomLeft**, **RadiusBottomRight**, **RadiusTopLeft**, **RadiusTopRight**): ```20```
- **Visible**: ```glbShowPopUpAddSpeaker```
- **X**: ```contMainBodyTranscriptHoriz.Width+contMainBodyTranscriptHoriz.X-Self.Width```
- **Y**: ```contMainBodyTranscriptHoriz.Y+contSpeakerTranscriptHoriz.Height+5```
  
**frmAddSpeaker** _(contPopUpAddSpeaker)_  
Submits new speaker name to Speakers table  
- **DataSource**: ```Speakers```
- **DefaultMode**: ```FormMode.New```
- **OnSelect**: Reset form and set glbShowPopUpAddSpeaker to false
  ```
  Set(
    glbShowPopUpAddSpeaker,
    false
  );
  ResetForm(frmAddSpeaker)
  ```

**btnAddSpeakerSave** _(contPopUpAddSpeaker->contPopUpAddSpeakerButtons)_  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/221e4be6-985b-468d-8d2d-be65ae77c297)

- **AccessibleLabel**: ```"Save the new speaker"```
- **DisplayMode**: If Name field is blank, disable button
  ```
  If(
    IsBlank(Name_DataCard_Value.Value),
    DisplayMode.Disabled,
    DisplayMode.Edit
  )
  ```
- **OnSelect**: ```SubmitForm(frmAddSpeaker);```
- **Text**: ```"Save"```

**btnAddSpeakerCancel** _(contPopUpAddSpeaker->contPopUpAddSpeakerButtons)_  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/f288d3da-a2be-4203-928c-625bb8efc987)
- **AccessibleLabel**: ```"Cancel adding the new speaker"```
- **Appearance**: ```'ButtonCanvas.Appearance'.Secondary```
- **OnSelect**: Reset form and hide the Add New Speaker pop-up
  ```
  Set(
    glbShowPopUpAddSpeaker,
    false
  );
  ResetForm(frmAddSpeaker)
  ```
- **Text**: ```"Cancel"```

**contPopUpUpdateAllSpeakersBg** <a name="contPopUpUpdateAllSpeakersBg"></a>
Full screen container that has an opqaue fill and is only visible when **gblShowPopUpUpdateAllSpeakers** = **true**
- **Fill**: ```RGBA(255, 255, 255, 0.65)```
- **Height**: Parent.Height
- **LayoutAlignItems**: ```LayoutAlignItems.Center```
- **LayoutJustifyContent**: ```LayoutJustifyContent.Center```
- **Visible**: ```gblShowPopUpUpdateAllSpeakers```
- **Width**: ```Parent.Width```


**contPopUpUpdateAllSpeakers** _(contPopUpUpdateAllSpeakersBg)_  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/341528f9-2ae8-46e5-92d5-63041fda8e0a)
- **DropShadow**: ```DropShadow.ExtraBold```
- **Fill**: ```RGBA(255, 255, 255, 1)```
- **Height**: ```Self.Width*.6```
- **LayoutAlignItems**: ```LayoutAlignItems.Center```
- **LayoutJustifyContent**: ```LayoutJustifyContent.Center```
- **Border Radius** (**RadiusBottomLeft**, **RadiusBottomRight**, **RadiusTopLeft**, **RadiusTopRight**): ```25```

**btnPopUpUpdateAllSpeakersYes**  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/5c28730e-97a4-47e8-840c-fec5aa5db699)
- **AccessibleLabel**: Dynamcially update accessible label based on the current phrase and the selected speaker name
  ```
  "Yes - For all the speakers equal to " &
  glbCurrentPhrase.demo_speaker &
  " please update Speaker (Lookup) to  " &
  drpSelectSpeaker_Transcript.Selected.Name
  ```
- **OnSelect**:
  ```
  //If user clicks Yes button
  //THEN hide the Update All Speakers pop-up
  Set(
      gblShowPopUpUpdateAllSpeakers,
      false
  );
  //THEN set flag (glbUpdateAllSpeakers) to true
  Set(
      gblUpdateAllSpeakers,
      true
  );
  //Then select the hidden button (with the actual save formulas)
  Select(btnSaveHidden);
  ```
- **Text**: ```"Yes"```

**btnPopUpUpdateAllSpeakersNo**  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/d0d07751-9058-4a01-aa02-44ad298e468f)  
- **AccessibleLabel**: ```"Please do not update all speakers to the selected speaker"```
- **Appearance**: ```'ButtonCanvas.Appearance'.Secondary```
- **OnSelect**:
  ```
  //IF user clicks No
  //THEN hide the Update All Speakers pop-up
  Set(
      gblShowPopUpUpdateAllSpeakers,
      false
  );
  //THEN set flag (gblUpdateAllSpeakers) to FALSE
  Set(
      gblUpdateAllSpeakers,
      false
  );
  //Then select the hidden button (with the actual save formulas)
  Select(btnSaveHidden);
  ```
- **Text**: ```"No"```

**btnSaveHidden**  
This button is hidden, but is called by various other buttons.  This is one technique to create reusable code/functions in Power Apps  
- **AccessibleLabel**: ```"This is a hidden control - used for saving the edits to the current phrase"```
- **OnSelect**: 
  ```
  /*
      This section of code is used to update the current recognized phrase.  
      Optionally: if the Speaker was updated, it also can loop through and update all instances of a Speaker Lookup for the selected Speaker. 
      This loop only happens if the user clicks Yes when the Update All Speakers popup appears
  */
  // Show Spinner
  Set(
      glbShowSpinner,
      true
  );
  //Patch the current recognized phrase with contents of the Current Phrase input (txtCurrentPhrase)
  Patch(
      'Recognized Phrases',
      LookUp(
          'Recognized Phrases',
          'Offset in Seconds' <= Int(audRecordingPlayback.Time) And Outset >= Round(
              audRecordingPlayback.Time,
              2
          )
      ),
      {
          Display: txtCurrentPhrase_Transcript.Value,
          'Speaker Lookup': drpSelectSpeaker_Transcript.Selected
      }
  );
  // If user selected "Yes" to Update All Speakers, then loop through and update every phrase record with selected speaker
  If(
      gblUpdateAllSpeakers,
      // Collect all Recognized Phrases that match the current speaker (e.g. 1)
      ClearCollect(
          colUpdateAllPhrasesForSelectedSpeaker,
          Filter(
              'Recognized Phrases',
              Speaker = LookUp(
                  colPhrases,
                  'Offset in Seconds' > Int(audRecordingPlayback.Time) And Outset > Round(
                      audRecordingPlayback.Time,
                      2
                  )
              ).Speaker
          )
      );
      //Then loop through the phrases (colUpdateAllPhrasesForSelectedSpeaker) and update Speaker Lookup to the currently selected speaker (drpSelectSpeaker)
      ForAll(
          colUpdateAllPhrasesForSelectedSpeaker As AllPhrases,
          Patch(
              'Recognized Phrases',
              LookUp(
                  'Recognized Phrases' As CurrentPhrase,
                  CurrentPhrase.'Recognized Phrases' = AllPhrases[@demo_recognizedphrasesid]
              ),
              {'Speaker Lookup': drpSelectSpeaker_Transcript.Selected}
          )
      )
  );
  //Do remaining functions concurrently to save time:
  Concurrent(
  //Reload the Recognized Phrases into a local collection (will playback performance, but will be slow to load for larger Transcripts)
      ClearCollect(
          colPhrases,
          SortByColumns(
              Filter(
                  'Recognized Phrases',
                  Transcript.Transcript = glbSelectedTranscript.Transcript
              ),
              "demo_offsetinseconds",
              SortOrder.Ascending
          )
      ),
  //Refresh Current Phrase (glbCurrentPhrase)
      Set(
          glbCurrentPhrase,
          LookUp(
              ShowColumns(
                  'Recognized Phrases',
                  "demo_display",
                  "demo_durationinseconds",
                  "demo_offsetinseconds",
                  "demo_outset",
                  "demo_phrasenumber",
                  "demo_speaker",
                  "demo_SpeakerLookup",
                  "demo_Transcript",
                  "demo_recognizedphrasesid"
              ),
              demo_offsetinseconds <= Trunc(audRecordingPlayback.Time) And demo_outset >= Trunc(audRecordingPlayback.Time)
          )
      ),
  //Reset variable (glbMode) to View Mode (DisplayMode.View
      Set(
          glbMode,
          DisplayMode.View
      ),
  //Reset Speaker Drop Down
      Reset(drpSelectSpeaker_Transcript)
  );
  //Hide Spinnner
  Set(
      glbShowSpinner,
      false
  );
  ```
- **Visible**: ```false```


[^Top](#contents)

*** 

## Flows
There are six flows in this solution.  They are designed to run sequentially (hence the numbering).   
[^Top](#contents)
### 01 - SPO - When Audio File Uploaded to SPO - Copy to Azure Blob
This flow is kicked off when the user uploads/attaches a file to the SharePoint list via the Power App.  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/0db99ce3-b1bc-4465-aef9-d32b148f1d82)

Note: SharePoint was used due a current limitation with the Azure Blob Storage connector in GCC-High (as of 3/20/24).  I recommend connecting directly to Azure Blob Storage from the canvas app if possible

Here's a detailed breakdown of each action:
- **When an item is created**: Monitors the SharePoint list for new items
- **Get attachments**: Retrieves all attachments for a particular item
- **Apply to each**: Loops through each attachment. Note: the app only allows for one upload at a time, but if you increase that limit, this flow will work
  - **Get attachment content**: Retrieves the binary content for the attached file (i.e. audio file)
  - **Create blob (V2)**: Creates a new blob in the specified container
    - _Note: You must have an Azure Storage account and container to use this (see [Prerequistes](transcript-demo-power%2Bazure#prerequisites))_

[^Top](#contents)
### 02 - Azure - When Audio File Created in Blob Storage - Create Transcript  
Master flow that is triggered when a file is uploaded to the Azure Blob storage container. Then it transcribes the audio file (via Azure Speech Services) and then loads that transcript into Dataverse and optionally, removes the source audio from the SP list.

For more on the Azure Batch Speech to Text transcription click [here]([url](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-transcription)): 
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/2a9d5968-8aa0-472e-b2b5-43dab942c610)

Here's breakdown of each action:
- **When a blob is added or modified (properties only) (V2)**: Flow is triggered when a new blob is created in the specified container
  - In this demo, the container is called "speech-to-text-demo" **You will need to update this trigger with your storage account and container**
- **Create SAS URI by path (V2)**: Creates a Shared Access String URI path with read-only permissions set to expire 1 year later.  
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/932db720-558c-4953-9c85-e3f828eecd8c)  

- **HTTP**: Due to limitations at the time of this writing, the solution leverages the [Azure Batch Speech to Text REST API](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/rest-speech-to-text#transcriptions) instead of the Azure Batch Speech to Text connector. I recommend re-factoring if/when possible to use OOTB connector when possible.
   
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/6a2b2f84-c9c6-41b9-be0f-7baf803e0ef4)  
   Here are the parameters passed:  
  - **Method**: ```POST```
  - **URI**: ```https://usgovvirginia.api.cognitive.microsoft.us/speechtotext/v3.1/transcriptions```
  - **Headers**:
    -   **Content-Type**: ```application/json```
    -   **Ocp-Apim-Subscription-Key**: ```@parameters('Speech To Text Key (demo_SpeechToTextKey)')```
    -   **Body**: 
      ```
      {
        "title": "Transcription",
        "model": null,
        "properties": {
          "diarizationEnabled": true,
          "wordLevelTimestampsEnabled": false,
          "displayFormWordLevelTimestampsEnabled": false,
          "channels": [
            0
          ],
          "destinationContainerUrl": "@{parameters('Azure Blob Destination SAS URL (demo_AzureBlobDestinationSASURL)')}",
          "punctuationMode": "None",
          "profanityFilterMode": "None",
          "diarization": {
            "speakers": {
              "minCount": 1,
              "maxCount": 4
            }
          }
        },
        "contentUrls": [
          "@{triggerBody()['text']}"
        ],
        "displayName": "test-@{utcNow()}",
        "locale": "en-US"
      }
      ```
      There are more options you can pass to the REST API. See full documentation [here](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/rest-speech-to-text#transcriptions)
- **Parse JSON**: Parses the body (output) of the HTTP action.
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/08198429-189a-4b8c-b8a2-3b878a4199f9)
   Here are the parameters:
  - **Content**: ```@{body('HTTP')}```
  - **Schema**
     ```
    {
        "type": "object",
        "properties": {
            "self": {
                "type": "string"
            },
            "model": {
                "type": "object",
                "properties": {
                    "self": {
                        "type": "string"
                    }
                }
            },
            "links": {
                "type": "object",
                "properties": {
                    "files": {
                        "type": "string"
                    }
                }
            },
            "properties": {
                "type": "object",
                "properties": {
                    "diarizationEnabled": {
                        "type": "boolean"
                    },
                    "wordLevelTimestampsEnabled": {
                        "type": "boolean"
                    },
                    "displayFormWordLevelTimestampsEnabled": {
                        "type": "boolean"
                    },
                    "email": {
                        "type": "string"
                    },
                    "channels": {
                        "type": "array",
                        "items": {
                            "type": "integer"
                        }
                    },
                    "punctuationMode": {
                        "type": "string"
                    },
                    "profanityFilterMode": {
                        "type": "string"
                    },
                    "diarization": {
                        "type": "object",
                        "properties": {
                            "speakers": {
                                "type": "object",
                                "properties": {
                                    "minCount": {
                                        "type": "integer"
                                    },
                                    "maxCount": {
                                        "type": "integer"
                                    }
                                }
                            }
                        }
                    }
                }
            },
            "lastActionDateTime": {
                "type": "string"
            },
            "status": {
                "type": "string"
            },
            "createdDateTime": {
                "type": "string"
            },
            "locale": {
                "type": "string"
            },
            "displayName": {
                "type": "string"
            }
        }
    }
    ```
  
- **Run a Child Flow - Loop Until Complete**: Calls the child flow () and passes the path (URL) of the transcriptions (from the previous child flow)
- **Run a Child Flow - Get Transcript Results**: Calls the child flow () and passes the path of the transcription files (from previous child flow)
- **Run a Child Flow - Parse Transcript and Load into Dataverse**: Calls the child flow () and passes the following parameters:  
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/3547c0d4-aa1a-4f60-9f27-fc859b26b7f2)

  - **Transcript**: ```string(outputs('Run_a_Child_Flow_-_Get_Transcript_Results')?['Body'])```
  - **File Name** ```@{triggerOutputs()?['body/DisplayName']}```
  - **File Size**: ```@{triggerOutputs()?['body/Size']}```
- **Get Items**: Get all items from the SharePoint list
  - _Note: this step is optional and will not be necessary if you write directly to Azure Blob from the canvas app (recommended)_
- **Apply to each**: Loop through each item in the SharePoint List (should only be 1 item)
  - **Delete item**: Delete each SharePoint list item
    
[^Top](#contents)

### 02b Child Flow - Loop Until Transcript Complete
Due to issue/limitation of the [Azure Blob Storage trigger]([url](https://learn.microsoft.com/en-us/connectors/azureblob/)) on file create/update, I had to create a flow that waits for the transcription to complete. Use caution when looping. If possible, re-factor to trigger when transcript file is completed.  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/cf3f6466-3fd5-416b-9699-df0fab9d6e9a)  
Here's a breakdown of each action:  
- **Manually trigger flow**: Child flow is triggered from parent flow [02 - Azure - When Audio File Created in Blob Storage - Create Transcript](#02---azure---when-audio-file-created-in-blob-storage---create-transcript) and receives a text parameter with the transcriptions path
- **Initialize variable  varWait**: Creates a variable with these paramters
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/116b2c78-df66-43a0-939d-8ec2a0f24501)

  - **Name**: ```varWait```
  - **Type**: ```Integer```
  - **Value**: ```500```  
    _Higher the number, the longer the wait_
- **Initialize variable varCompleted**: Creates variable with these parameters:
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/dd18c404-2229-4e65-ab61-cac2eeb6525e)

  - **Name**: ```varCompleted```
  - **Type**: ```Boolean```
  - **Value**: ```@{false}```
- **Do until**:  This loops until **varComplete** is **true**. 
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/1e4d24fa-20e9-4860-aa92-f2d3996b4960)
   Inside the following actions happen for each loop:
  - **Reset variable varWait**: At the start of each loop, reset to 500  - 
  - **HTTP Get Transcript Status**:  Attempts to retrieve the transcription status using the [Azure Batch Speech to Text REST API](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/rest-speech-to-text#transcriptions).  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/89475946-26fd-4464-9f31-fc1d23c480db)  
    With the following parameters:
    - **Method**: ```GET```
    - **URI**: ```@{triggerBody()['text']}```
      _This is the path passed to the flow from the parent flow  [02 - Azure - When Audio File Created in Blob Storage - Create Transcript](#02---azure---when-audio-file-created-in-blob-storage---create-transcript)_
    - **Headers**:
      -  **Ocp-Apim-Subscription-Key**: ```@parameters('Speech To Text Key (demo_SpeechToTextKey)')```
  - **If Fail, wait and try again**: A second Do Until loop only runs when the previous action fails.  The HTTP action fails until the transcription has started. This can take several minutes depending on Azure resources.
    The loop ends when varWait equals 0  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/8843295d-a675-4773-8b4f-8666af01cf7d)  
    _Note: the Configure Run After is set to only run this action when the previous action fails_  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/d16a3d5e-d379-4dc5-bce5-10cdc2ada818)  
  - **Parse JSON**: This action (and subsequent actions) only run when the previous action is skipped.  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/a5b034fd-938d-4c72-9683-1642d1a16df6)  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/e08d8dcd-dfeb-4e42-8025-43743c8af9f6)  

    The follow parameters are passed:  
    - **Content**: Pass the output of the **HTTP Get Transcript Status** action
      ```@{body('HTTP_Get_Transcript_Status')}```
    - **Schema**:
      ```
      {
          "type": "object",
          "properties": {
              "self": {
                  "type": "string"
              },
              "model": {
                  "type": "object",
                  "properties": {
                      "self": {
                          "type": "string"
                      }
                  }
              },
              "links": {
                  "type": "object",
                  "properties": {
                      "files": {
                          "type": "string"
                      }
                  }
              },
              "properties": {
                  "type": "object",
                  "properties": {
                      "diarizationEnabled": {
                          "type": "boolean"
                      },
                      "wordLevelTimestampsEnabled": {
                          "type": "boolean"
                      },
                      "displayFormWordLevelTimestampsEnabled": {
                          "type": "boolean"
                      },
                      "channels": {
                          "type": "array",
                          "items": {
                              "type": "integer"
                          }
                      },
                      "punctuationMode": {
                          "type": "string"
                      },
                      "profanityFilterMode": {
                          "type": "string"
                      },
                      "duration": {
                          "type": "string"
                      },
                      "languageIdentification": {
                          "type": "object",
                          "properties": {
                              "candidateLocales": {
                                  "type": "array",
                                  "items": {
                                      "type": "string"
                                  }
                              }
                          }
                      }
                  }
              },
              "lastActionDateTime": {
                  "type": "string"
              },
              "status": {
                  "type": "string"
              },
              "createdDateTime": {
                  "type": "string"
              },
              "locale": {
                  "type": "string"
              },
              "displayName": {
                  "type": "string"
              }
          }
      }
      ```
  - **If Status is Success**: A conditional control that checks if the status is Succeeded  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/e427bb29-fb80-45f5-9270-c60e80538695)  
    - **If Yes**: Then update **varCompleted** to ```true```
      ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/20a888d9-ee4e-4771-a902-43209066957c)
    - If no: Reset the varWait and do another loop (do until):  
      ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/fd7aa5ef-23cd-4540-aa9c-e86e841d227a)  

      - **Reset varWait to 5000**
      - **If not Succeeded, Wait and Try Again**: Do Until **varWait** equals ```0```
        -  **Decrement variable  varWait by 1**


[^Top](#contents)
### 02c Child Flow - Get Transcript Results
Use [Azure Speech Services REST API](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/rest-speech-to-text#transcriptions) to retrieve the transcription files (report and content) using the Path provided by previous flow. 
NOTE: Due to issues with OOTB Azure Speech Services connector, I leveraged the HTTP connector to call the Azure Speech Services REST API. I recommend re-factoring to use the OOTB connector if/when possible  
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/1c282b5c-d74d-47f7-9f9a-3a6a27ab6129)  
Here is a breakdown of each action:
- **Manually trigger a flow**:  Child flow is triggered from parent flow [02 - Azure - When Audio File Created in Blob Storage - Create Transcript](#02---azure---when-audio-file-created-in-blob-storage---create-transcript) and receives a text parameter with the transcription files path
- **HTTP Get Transcript Files**: Gets the files generated by the Azure Batch Speech to Text transcription service (from flow )  
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/26892f48-6ff8-4609-a0a1-8bbfde1e8b95)  
  Here are the parameters passed:
  - **Method**: ```GET```
  - **URI**: ```@{triggerBody()['text']}```
  - **Headers**:
      -  **Ocp-Apim-Subscription-Key**: ```@parameters('Speech To Text Key (demo_SpeechToTextKey)')```
- **Parse JSON**: Takes the output (JSON) from the previous action and parses it
  - **Content**: ```@{body('HTTP_Get_Transcript_Files')}```
  - **Shema**:
    ```
    {
        "type": "object",
        "properties": {
            "values": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "self": {
                            "type": "string"
                        },
                        "name": {
                            "type": "string"
                        },
                        "kind": {
                            "type": "string"
                        },
                        "properties": {
                            "type": "object",
                            "properties": {
                                "size": {
                                    "type": "integer"
                                }
                            }
                        },
                        "createdDateTime": {
                            "type": "string"
                        },
                        "links": {
                            "type": "object",
                            "properties": {
                                "contentUrl": {
                                    "type": "string"
                                }
                            }
                        }
                    },
                    "required": [
                        "self",
                        "name",
                        "kind",
                        "properties",
                        "createdDateTime",
                        "links"
                    ]
                }
            }
        }
    }
    ```
  - **Filter array - contenturl 0.json**: Filters the array of files returned to the one (contenturl_0.json) with the actual transcript  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/10d067f2-7320-41db-b76e-8e6fe75a5fc6)
  - **HTTP Get Transcript**: Get's the actual transcript (JSON) via the REST API  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/b808a290-4a1d-4a67-a1df-777ae1322e5c)  
    Here are the parameters passed:
    - **Method**: ```GET```
    - **URI**: ```@{first(body('Filter_array_-_contenturl_0.json'))?['links']?['contenturl']}```
      **_Note_**: _This flow uses the ```first()``` function to avoid the need for For Each loop. IF you are submitting mutliple files at one time to transcribe, please replace with a For Each control_  
    - **Headers**:
        -  **Ocp-Apim-Subscription-Key**: ```@parameters('Speech To Text Key (demo_SpeechToTextKey)')```
  - **Response**: Sends the full transcript JSON back to the parent flow.  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/1c7045dc-fe01-4cb7-acc5-3b852f32dacb)  

    The paramters are:
    - **Status Code**: ```200```
    - **Body**: ```@{body('HTTP_Get_Transcript')}```
    - **Response Body JSON Schema**:
      ```
      {
          "type": "object",
          "properties": {
              "source": {
                  "type": "string"
              },
              "timestamp": {
                  "type": "string"
              },
              "durationInTicks": {
                  "type": "integer"
              },
              "duration": {
                  "type": "string"
              },
              "combinedRecognizedPhrases": {
                  "type": "array",
                  "items": {
                      "type": "object",
                      "properties": {
                          "channel": {
                              "type": "integer"
                          },
                          "lexical": {
                              "type": "string"
                          },
                          "itn": {
                              "type": "string"
                          },
                          "maskedITN": {
                              "type": "string"
                          },
                          "display": {
                              "type": "string"
                          }
                      },
                      "required": [
                          "channel",
                          "lexical",
                          "itn",
                          "maskedITN",
                          "display"
                      ]
                  }
              },
              "recognizedPhrases": {
                  "type": "array",
                  "items": {
                      "type": "object",
                      "properties": {
                          "recognitionStatus": {
                              "type": "string"
                          },
                          "channel": {
                              "type": "integer"
                          },
                          "speaker": {
                              "type": "integer"
                          },
                          "offset": {
                              "type": "string"
                          },
                          "duration": {
                              "type": "string"
                          },
                          "offsetInTicks": {
                              "type": "integer"
                          },
                          "durationInTicks": {
                              "type": "integer"
                          },
                          "nBest": {
                              "type": "array",
                              "items": {
                                  "type": "object",
                                  "properties": {
                                      "confidence": {
                                          "type": "number"
                                      },
                                      "lexical": {
                                          "type": "string"
                                      },
                                      "itn": {
                                          "type": "string"
                                      },
                                      "maskedITN": {
                                          "type": "string"
                                      },
                                      "display": {
                                          "type": "string"
                                      }
                                  },
                                  "required": [
                                      "confidence",
                                      "lexical",
                                      "itn",
                                      "maskedITN",
                                      "display"
                                  ]
                              }
                          }
                      },
                      "required": [
                          "recognitionStatus",
                          "channel",
                          "speaker",
                          "offset",
                          "duration",
                          "offsetInTicks",
                          "durationInTicks",
                          "nBest"
                      ]
                  }
              }
          }
      }
      ```

[^Top](#contents)
### 02d Child Flow - Parse Transcript and Load into Dataverse  
Parses the transcript file and loads into Dataverse. One record in the Transcripts table for the transcription and records in the Recognized Phrases for each phrase returned by Azure Speech Services
![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/17200620-119d-4a0e-8465-371648e26579)

Here is a breakdown of eacha action:
- **Manually trigger a flow**:  Child flow is triggered from parent flow [02 - Azure - When Audio File Created in Blob Storage - Create Transcript](#02---azure---when-audio-file-created-in-blob-storage---create-transcript) and receives three parameters  
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/e0177581-1daf-417b-99a4-6cd87984af3a)  
  - **Transcript**
  - **File Name**
  - **File Size**
- **Parse JSON**  
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/0c27ca9d-3f9a-4e14-bcc9-2b70ba966e2d)  
  - **Content**: ```@{triggerBody()['text']}```
  - **Schema**:
    ```
    {
        "type": "object",
        "properties": {
            "source": {
                "type": "string"
            },
            "timestamp": {
                "type": "string"
            },
            "durationInTicks": {
                "type": "number"
            },
            "recognizedPhrases": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "recognitionStatus": {
                            "type": "string"
                        },
                        "channel": {
                            "type": "integer"
                        },
                        "speaker": {
                            "type": "integer"
                        },
                        "offsetInTicks": {
                            "type": "number"
                        },
                        "durationInTicks": {
                            "type": "number"
                        },
                        "nBest": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "confidence": {
                                        "type": "number"
                                    },
                                    "display": {
                                        "type": "string"
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
    ```
  - **Add a new row**: Adds a new row to the Transcripts table  
  ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/416c5a63-ec8b-4c81-bed5-77d70912aa65)  
  With the following parameters: 
    - **Table name**: ```Transcripts```
    - **Duration**: ```@{div(int(body('Parse_JSON')?['durationInTicks']),10000000.00)}```
      - _Note: Uses ```div()``` and ```int()``` functions to return the duration in seconds with two decimal points_
    - **Duration in Ticks**: ```@{int(body('Parse_JSON')?['durationInTicks'])}```
      - _Note: Uses ```int()``` to convert **durationInTicks** from **Parse JSON** into integer_ 
    - **Source File Name**: ```@{triggerBody()['text_1']}```
    - **Source File Size**: ```@{triggerBody()['number']}```
    - **Source URL**: ```@{body('Parse_JSON')?['source']}```
    - **Time Stamp**: ```@{body('Parse_JSON')?['timestamp']}```
- **Apply to each**: For each Recognized Phrase (from Parse JSON), perform the following action:
  - **Add a new row to Recognized Phrases**: Add each recogonized phrase to the Recogonized Phrases table  
    ![image](https://github.com/microsoft/Federal-Business-Applications/assets/12347531/9a6baf65-f178-450d-bc2a-13ef28e0b4b0)  
    with the following parameters:
    - **Table name**: ```Recognized Phrases```
    - **Confidence**: ```@{first(items('Apply_to_each')['nBest'])?['confidence']}```
      - _Note: The ```first()``` function is used to avoid another For Each loop. See [Rest API documentation](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-transcription-get?pivots=rest-api#transcription-result-file) for more on **nBest**_
    - **Display**: ```@{first(items('Apply_to_each')['nBest'])?['display']}```
      - _Note: The ```first()``` function is used to avoid another For Each loop. See [Rest API documentation](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-transcription-get?pivots=rest-api#transcription-result-file) for more on **nBest**_
    - **Duration in Seconds**: ```@{div(int(items('Apply_to_each')?['durationInTicks']),10000000.00)```
      - _Note: Uses ```div()``` and ```int()``` functions to return the duration in seconds with two decimal points_
    - **Duration in Ticks**: ```@{int(items('Apply_to_each')?['durationInTicks'])}```
      - _Note: Uses ```int()``` to convert **durationInTicks** from **Parse JSON** into integer_ 
    - **Offset in Seconds**: ```@{div(int(items('Apply_to_each')?['offsetInTicks']),10000000.00)}```
      - _Note: Uses ```div()``` and ```int()``` functions to return the duration in seconds with two decimal points_
    - **Offset in Ticks**: ```@{int(items('Apply_to_each')?['offsetInTicks'])}```
      - _Note: Uses ```int()``` to convert **offsetInTicks** from **Parse JSON** into integer_ 
    - **Speaker**: ```@{items('Apply_to_each')?['speaker']}```
    - **Transcript (Transcripts)**: ```demo_transcripts(@{outputs('Add_a_new_row')?['body/demo_transcriptid']})```
      - _Note: This relates the recognized phrase to it's parent record in the Transcripts table_
- **Response**: Returns **Status**: ```200``` to parent flow if no issues/errors.
  
[^Top](#contents)
